<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Hyper Ontoverse 2: Partial Observations, Multi-Skill Synergy, Basic Communication</title>
  <style>
    body {
      margin: 0; padding: 0;
      background: #1a1a1a; color: #fff;
      font-family: Arial, sans-serif;
      overflow: hidden;
    }
    #controls {
      position: absolute; top: 10px; left: 10px;
      background: rgba(0,0,0,0.7);
      padding: 10px; border-radius: 6px; 
      z-index: 10;
    }
    #controls label, #controls button {
      display: block; margin: 5px 0;
    }
    #performance {
      position: absolute; bottom:10px; right:10px;
      background: rgba(0,0,0,0.7);
      padding: 10px; border-radius:6px;
      z-index: 10;
    }
    #performance p { margin:5px 0; font-size:0.9em; }
    #eventLog {
      position: absolute; top:200px; left:10px;
      background: rgba(0,0,0,0.7);
      width:270px; max-height:280px; 
      overflow-y:auto; border-radius:6px;
      padding:10px; z-index:10;
    }
    #eventLog h3 { margin:0 0 5px; font-size:1em; }
    #eventLog ul { list-style-type:none; margin:0; padding:0; font-size:0.85em; }
    #eventLog li { margin-bottom:4px; }
  </style>
</head>
<body>
<div id="controls">
  <label>Agent Speed:</label>
  <input type="range" id="speedSlider" min="1" max="10" value="5">
  
  <label>Threat Aggressiveness:</label>
  <input type="range" id="threatSlider" min="1" max="5" value="3">

  <button onclick="resetSimulation()">Reset Simulation</button>
  <button onclick="createSimpleTask()">Spawn Simple Task</button>
</div>

<div id="performance">
  <p>FPS: <span id="fps">0</span></p>
  <p>Step Time: <span id="stepTime">0</span> ms</p>
  <p>Commander Step: <span id="commanderStep">0</span></p>
  <p>Time of Day: <span id="timeOfDay">0</span> hr</p>
</div>

<div id="eventLog">
  <h3>Event Log</h3>
  <ul id="eventList"></ul>
</div>

<canvas id="canvas"></canvas>

<!-- TensorFlow.js from CDN -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
<script>
/*****************************************************************
  Memory snippet for GitHub Title & Description
*****************************************************************/
const memory = [];
const githubTitle = "Multi-Agent Ontoverse RL (Commander + Dueling DQN + Day/Night + Multi-Skill Synergy)";
const githubDescription = `
Extends the multi-agent environment with partial observations, multi-skill synergy tasks, 
basic agent communication, advanced Commander logic, day/night cycles, knowledge graph placeholders, 
and a memory snippet for GitHub details. Runs on TensorFlow.js with Dueling DQNs.
`;
memory.push({title: githubTitle, description: githubDescription});
console.log("Memory snippet =>", memory[0]);

/*****************************************************************
  Canvas Setup
*****************************************************************/
const canvas = document.getElementById("canvas");
const ctx = canvas.getContext("2d");
canvas.width = window.innerWidth;
canvas.height= window.innerHeight;
window.addEventListener("resize", ()=>{
  canvas.width= window.innerWidth;
  canvas.height= window.innerHeight;
});

/*****************************************************************
  Global Settings
*****************************************************************/
const settings = {
  agent: {
    count: 10,
    maxSpeed: 5,
    maxHealth: 140,
    memorySize: 300,
    batchSize: 32,
    gamma: 0.99,
    epsilon: 1.0,
    epsilonMin: 0.05,
    epsilonDecay: 0.995,
    lr: 0.001,

    skillCount: 3, // # of knowledge topics
    partialObsRange: 200, // how far an agent can see
  },
  threat: {
    count: 5,
    baseSpeed: 2,
    aggressiveness:3
  },
  food: {
    count: 10
  },
  tasks: {
    count: 4,
    synergyRequired: 2,
    synergyReward: 40,
    normalReward: 20,
    multiStep: true
  },
  dayNight: {
    enabled: true,
    timeRate: 0.02
  },
  knowledgeGraph: {
    nodes: ["healingFood","dangerThreat","synergyTask"],
    edges: [
      {source:"healingFood", target:"dangerThreat"},
      {source:"synergyTask", target:"healingFood"}
    ]
  },
  eventLogSize: 140
};

/*****************************************************************
  Entities
*****************************************************************/
class Entity {
  constructor(x,y,size,color){
    this.x=x; this.y=y;
    this.size=size; this.color=color;
    this.alive=true;
  }
  draw(){
    ctx.fillStyle= this.color;
    ctx.beginPath();
    ctx.arc(this.x,this.y,this.size,0,2*Math.PI);
    ctx.fill();
  }
}

class Agent extends Entity {
  constructor(x,y){
    super(x,y,6,"blue");
    this.health= settings.agent.maxHealth;
    this.speed= Math.random()* settings.agent.maxSpeed +1;
    this.brain= buildDuelingDQN(getAgentInputSize(),4);
    this.targetBrain= buildDuelingDQN(getAgentInputSize(),4);
    this.targetBrain.setWeights(this.brain.getWeights());
    this.experiences=[];
    this.epsilon= settings.agent.epsilon;

    // skill vector
    this.topicSkills= [
      Math.random(), 
      Math.random(),
      Math.random()
    ];

    // for partial comm:
    this.commBuffer= [];
  }

  step(agents, threats, foods, tasks, timeOfDay){
    // partial observation => gather local state
    let state= this.observeLocal(agents, threats, foods, tasks, timeOfDay);

    let action;
    if(Math.random()< this.epsilon){
      action= Math.floor(Math.random()*4);
    } else {
      let stT= tf.tensor2d([state]);
      let qVals= this.brain.predict(stT);
      let arr= qVals.dataSync();
      qVals.dispose(); stT.dispose();
      action= argmax(arr);
    }
    let dx=0, dy=0;
    switch(action){
      case 0: dy=-this.speed; break; // up
      case 1: dy= this.speed; break; // down
      case 2: dx=-this.speed; break; // left
      case 3: dx= this.speed; break; // right
      default: break;
    }
    this.x+= dx; this.y+= dy;
    keepInBounds(this);

    let reward= this.interact(threats, foods, tasks, timeOfDay);
    let nextState= this.observeLocal(agents, threats, foods, tasks, timeOfDay);
    this.experiences.push({state, action, reward, nextState, done:!this.alive});

    // share experiences with near agents
    this.shareExperiences(agents);

    if(this.experiences.length> settings.agent.batchSize*2){
      this.learn();
    }
    if(this.epsilon> settings.agent.epsilonMin){
      this.epsilon*= settings.agent.epsilonDecay;
    }
  }

  // partial obs => gather nearest threat,food,task within partialObsRange
  observeLocal(agents, threats, foods, tasks, timeOfDay){
    const range= settings.agent.partialObsRange;
    let nearestThreat= findNearestInRange(this, threats, range);
    let nearestFood  = findNearestInRange(this, foods, range);
    let nearestTask  = findNearestInRange(this, tasks, range);

    let tX=0, tY=0, fX=0, fY=0, taX=0, taY=0;
    if(nearestThreat){ 
      tX= (nearestThreat.x - this.x)/range;
      tY= (nearestThreat.y - this.y)/range;
    }
    if(nearestFood){
      fX= (nearestFood.x - this.x)/range;
      fY= (nearestFood.y - this.y)/range;
    }
    if(nearestTask){
      taX= (nearestTask.x - this.x)/range;
      taY= (nearestTask.y - this.y)/range;
    }
    let night= (timeOfDay>18|| timeOfDay<6)?1:0;
    let normHealth= this.health/ settings.agent.maxHealth;

    return [
      tX, tY, 
      fX, fY,
      taX, taY,
      normHealth,
      night,
      this.topicSkills[0],
      this.topicSkills[1],
      this.topicSkills[2]
    ];
  }

  interact(threats, foods, tasks, timeOfDay){
    let r=-0.1;
    for(let i= threats.length-1;i>=0;i--){
      let t= threats[i];
      if(dist(this,t)< this.size+ t.size){
        this.health-=25; 
        r-=10;
        if(this.health<=0){
          this.alive=false; 
          logEvent("Agent died from threat!");
        }
      }
    }
    for(let i= foods.length-1;i>=0;i--){
      let f= foods[i];
      if(dist(this,f)< this.size+f.size){
        // skill synergy for "healingFood" => index=0
        let skill= this.topicSkills[0];
        let skillBonus= skill*5;
        this.health= Math.min(settings.agent.maxHealth, this.health + 20 + skillBonus);
        let rew= 5 + skillBonus;
        r+= rew;
        foods.splice(i,1);
        logEvent(`Agent ate food +${rew.toFixed(1)}`);
      }
    }
    // synergy tasks multi-step
    for(let i= tasks.length-1;i>=0;i--){
      let tk= tasks[i];
      if(!tk.stepCount) tk.stepCount= tk.steps? tk.steps:1;
      if(!tk.hasOwnProperty("steps") && settings.tasks.multiStep) {
        tk.steps= 1+ Math.floor(Math.random()*3);
        tk.stepCount= tk.steps;
        tk.synergyNeeded= settings.tasks.synergyRequired;
        // multi skill synergy -> random 1..2 topics required
        tk.requiredTopics= [];
        let howMany= 1+ Math.floor(Math.random()*2);
        for(let x=0;x< howMany;x++){
          let topID= Math.floor(Math.random()*3);
          if(!tk.requiredTopics.includes(topID)) tk.requiredTopics.push(topID);
        }
      }
      if(dist(this, tk)< this.size+ tk.size) {
        if(!tk.synergySet) tk.synergySet= new Set();
        tk.synergySet.add(this);

        if(tk.synergySet.size>= tk.synergyNeeded) {
          // check if among these agents we have all requiredTopics
          let hasAll= true;
          for(const neededTop of tk.requiredTopics){
            let foundAgent= false;
            for(const ag of tk.synergySet){
              if(ag.topicSkills[neededTop]> 0.5){ 
                foundAgent= true; 
                break;
              }
            }
            if(!foundAgent){
              hasAll= false; 
              break;
            }
          }
          if(hasAll){
            // pass step
            tk.stepCount--;
            let skillBoost= tk.requiredTopics.length*3; 
            let rew= 10+ skillBoost;
            r+= rew;
            logEvent(`Task step done (multi-skill). neededTopics=${tk.requiredTopics}, +${rew}`);

            if(tk.stepCount<=0){
              let dayBonus= (timeOfDay>=6&& timeOfDay<=18)? 5:0;
              let finalRew= settings.tasks.synergyReward+ dayBonus;
              r+= finalRew;
              logEvent(`Agent synergy final step => +${finalRew}`);
              tasks.splice(i,1);
            } else {
              // reset synergy set for next step
              tk.synergySet= new Set();
            }
          } else {
            // partial synergy but missing a skill => no pass
          }
        }
      }
    }
    // small drain
    this.health-= 0.05;
    if(this.health<=0){
      this.alive=false; 
      logEvent("Agent died from health depletion.");
      r-=5;
    }
    return r;
  }

  shareExperiences(agents) {
    // find agents within 50 px
    for(const other of agents){
      if(other===this) continue;
      if(dist(this, other)<50) {
        // share a small random sample
        let count= Math.min(3, this.experiences.length);
        for(let i=0;i< count; i++){
          let idx= Math.floor(Math.random()* this.experiences.length);
          other.experiences.push(this.experiences[idx]);
        }
      }
    }
  }

  learn(){
    if(this.experiences.length< settings.agent.batchSize) return;
    let batch= [];
    for(let i=0;i< settings.agent.batchSize;i++){
      let idx= Math.floor(Math.random()* this.experiences.length);
      batch.push(this.experiences[idx]);
    }
    const states=[], nextStates=[], actions=[], rewards=[], dones=[];
    for(const ex of batch){
      states.push(ex.state);
      nextStates.push(ex.nextState);
      actions.push(ex.action);
      rewards.push(ex.reward);
      dones.push(ex.done?1:0);
    }
    let sT= tf.tensor2d(states);
    let nsT= tf.tensor2d(nextStates);

    let qNextLocal= this.brain.predict(nsT);
    let qNextTarget= this.targetBrain.predict(nsT);
    let localArr= qNextLocal.arraySync();
    let targArr= qNextTarget.arraySync();

    let currPred= this.brain.predict(sT);
    let currArr= currPred.arraySync();

    for(let i=0;i< batch.length;i++){
      let bestA= argmax(localArr[i]);
      let target= dones[i]? rewards[i] : (rewards[i] + settings.agent.gamma* targArr[i][bestA]);
      currArr[i][ actions[i]] = target;
    }
    let updated= tf.tensor2d(currArr,[batch.length,4]);
    this.brain.fit(sT, updated, {epochs:1, verbose:0}).then(()=>{
      sT.dispose(); nsT.dispose();
      qNextLocal.dispose(); qNextTarget.dispose();
      currPred.dispose(); updated.dispose();
    });
  }

  updateTarget(){
    this.targetBrain.setWeights(this.brain.getWeights());
  }
}

class Threat extends Entity {
  constructor(x,y){
    super(x,y,8,"red");
  }
  move(timeOfDay){
    let sp= settings.threat.baseSpeed* settings.threat.aggressiveness;
    if(timeOfDay>18|| timeOfDay<6) sp*=1.2;
    this.x+= (Math.random()-0.5)* sp;
    this.y+= (Math.random()-0.5)* sp;
    keepInBounds(this);
  }
}
class Food extends Entity {
  constructor(x,y){ super(x,y,5,"green"); }
}
class Task extends Entity {
  constructor(x,y){ super(x,y,6,"yellow"); }
}

/*****************************************************************
  Commander Agent
*****************************************************************/
class CommanderAgent {
  constructor(){
    this.stepCount=0;
    this.epsilon=1.0;
    this.exp=[];
    this.model= buildCommanderDQN();
    this.targetModel= buildCommanderDQN();
    this.targetModel.setWeights( this.model.getWeights() );
  }

  step(agents, threats, foods, tasks, timeOfDay){
    this.stepCount++;
    // #agents, #tasks, synergy, timeOfDay, threatAggro => 5 input
    let st= [
      agents.length/30,
      tasks.length/10,
      settings.tasks.synergyRequired/5,
      timeOfDay/24,
      settings.threat.aggressiveness/5
    ];
    let action;
    if(Math.random()< this.epsilon){
      action= Math.floor(Math.random()*5); // 5 actions
    } else {
      let stT= tf.tensor2d([st]);
      let out= this.model.predict(stT);
      let arr= out.dataSync();
      out.dispose(); stT.dispose();
      action= argmax(arr);
    }
    let reward=0;
    switch(action){
      case 0:
        // spawn multi-step synergy tasks
        if(Math.random()<0.3){
          spawnMultiStepCommanderTask();
          reward+=2;
        }
        break;
      case 1:
        // lower synergy
        if(settings.tasks.synergyRequired>1){
          settings.tasks.synergyRequired--;
          logEvent("Commander lowered synergy requirement!");
          reward+=2;
        }
        break;
      case 2:
        // raise synergy
        if(settings.tasks.synergyRequired<5){
          settings.tasks.synergyRequired++;
          logEvent("Commander raised synergy requirement!");
          reward-=1;
        }
        break;
      case 3:
        // update agent target
        for(const ag of agents) { ag.updateTarget(); }
        reward+=3;
        logEvent("Commander forced target update on agents!");
        break;
      case 4:
        // random environment changes
        // spawn or remove a threat
        if(Math.random()<0.4 && threats.length>2){
          threats.pop();
          logEvent("Commander removed a threat!");
          reward+=2;
        } else {
          let x= Math.random()* canvas.width; 
          let y= Math.random()* canvas.height;
          threats.push(new Threat(x,y));
          logEvent("Commander spawned an extra threat!");
          reward-=2;
        }
        break;
      default: break;
    }

    let nextSt= [
      agents.length/30,
      tasks.length/10,
      settings.tasks.synergyRequired/5,
      timeOfDay/24,
      settings.threat.aggressiveness/5
    ];
    let done=false;
    this.exp.push({st, action, reward, nextSt, done});
    if(this.exp.length>200) this.learn();
    if(this.epsilon> 0.05) this.epsilon*= 0.999;
  }

  learn(){
    if(this.exp.length<64)return;
    let batch=[];
    for(let i=0;i<64;i++){
      let idx= Math.floor(Math.random()* this.exp.length);
      batch.push(this.exp[idx]);
    }
    const states=[], actions=[], rewards=[], nextStates=[], dones=[];
    for(const ex of batch){
      states.push(ex.st);
      actions.push(ex.action);
      rewards.push(ex.reward);
      nextStates.push(ex.nextSt);
      dones.push(ex.done?1:0);
    }
    let sT= tf.tensor2d(states);
    let nsT= tf.tensor2d(nextStates);

    let qLocal= this.model.predict(nsT);
    let qTarg= this.targetModel.predict(nsT);
    let locArr= qLocal.arraySync();
    let tarArr= qTarg.arraySync();

    let currPred= this.model.predict(sT);
    let currArr= currPred.arraySync();

    for(let i=0;i< batch.length;i++){
      let bestA= argmax(locArr[i]);
      let target= dones[i]? rewards[i] : (rewards[i]+ 0.98* tarArr[i][bestA]);
      currArr[i][actions[i]]= target;
    }
    let upd= tf.tensor2d(currArr,[batch.length,5]);
    this.model.fit(sT, upd, {epochs:1, verbose:0}).then(()=>{
      sT.dispose(); nsT.dispose();
      qLocal.dispose(); qTarg.dispose();
      currPred.dispose(); upd.dispose();
    });
    if(Math.random()<0.03){
      this.targetModel.setWeights(this.model.getWeights());
    }
  }
}

/*****************************************************************
  Build Commander Net
*****************************************************************/
function buildCommanderDQN(){
  // input shape =5, output=5
  const inp= tf.input({shape:[5]});
  let x= tf.layers.dense({units:32, activation:'relu'}).apply(inp);
  x= tf.layers.dense({units:16, activation:'relu'}).apply(x);
  let out= tf.layers.dense({units:5, activation:'linear'}).apply(x);
  let model= tf.model({inputs:inp, outputs:out});
  model.compile({optimizer: tf.train.adam(0.0005), loss:'meanSquaredError'});
  return model;
}

/*****************************************************************
  Additional Utility
*****************************************************************/
function logEvent(msg){
  const ul= document.getElementById("eventList");
  const li= document.createElement("li");
  li.textContent= `${new Date().toLocaleTimeString()}: ${msg}`;
  ul.appendChild(li);
  if(ul.children.length> settings.eventLogSize){
    ul.removeChild(ul.firstChild);
  }
  ul.scrollTop= ul.scrollHeight;
}
function argmax(arr){
  let max=-Infinity, idx=0;
  for(let i=0;i<arr.length;i++){
    if(arr[i]>max){
      max=arr[i]; idx=i;
    }
  }
  return idx;
}
function dist(a,b){
  let dx=a.x-b.x; let dy=a.y-b.y;
  return Math.hypot(dx,dy);
}
function keepInBounds(e){
  if(e.x< e.size) e.x=e.size;
  if(e.x> canvas.width- e.size)e.x= canvas.width- e.size;
  if(e.y< e.size) e.y=e.size;
  if(e.y> canvas.height- e.size)e.y=canvas.height- e.size;
}
function findNearest(ref, arr){
  if(arr.length<1)return null;
  let minD= Infinity, near=null;
  for(const x of arr){
    let dd= dist(ref,x);
    if(dd<minD){minD=dd; near=x;}
  }
  return near;
}
// partial observation version
function findNearestInRange(ref, arr, range){
  let candidates= arr.filter(a=> dist(ref,a)<= range);
  if(candidates.length<1) return null;
  let minD=Infinity; let near=null;
  for(const c of candidates){
    let dd= dist(ref,c);
    if(dd< minD){minD= dd; near=c;}
  }
  return near;
}
function getAgentInputSize(){
  // partial obs => we do 11 => threatX,threatY,foodX,foodY,taskX,taskY,health,night + 3 skill
  return 8 + settings.agent.skillCount;
}

// Basic Dueling DQN for agent
function buildDuelingDQN(inputSize, outputSize){
  const inp= tf.input({shape:[inputSize]});
  let x= tf.layers.dense({units:64, activation:'relu'}).apply(inp);
  x= tf.layers.dense({units:32, activation:'relu'}).apply(x);

  let value= tf.layers.dense({units:1, activation:'linear'}).apply(x);
  let advantage= tf.layers.dense({units:outputSize, activation:'linear'}).apply(x);

  let qVals= tf.layers.add().apply([value, advantage]);

  let model= tf.model({inputs:inp, outputs:qVals});
  model.compile({optimizer: tf.train.adam(settings.agent.lr), loss:'meanSquaredError'});
  return model;
}

/*****************************************************************
  Main
*****************************************************************/
let agents=[], threats=[], foods=[], tasks=[];
let commander;
let lastTimestamp= performance.now();
let fps=0, stepTime=0;
let timeOfDay=8;
const dayLength=24;

function initializeSimulation(){
  agents=[]; threats=[]; foods=[]; tasks=[];
  commander= new CommanderAgent();

  for(let i=0; i< settings.agent.count; i++){
    let x= Math.random()* canvas.width;
    let y= Math.random()* canvas.height;
    agents.push(new Agent(x,y));
  }
  for(let i=0; i< settings.threat.count; i++){
    let x= Math.random()* canvas.width;
    let y= Math.random()* canvas.height;
    threats.push(new Threat(x,y));
  }
  for(let i=0; i< settings.food.count; i++){
    let x= Math.random()*canvas.width;
    let y= Math.random()*canvas.height;
    foods.push(new Food(x,y));
  }
  for(let i=0; i< settings.tasks.count; i++){
    spawnMultiStepCommanderTask();
  }
  logEvent("Ultra environment with partial obs, multi-skill synergy, basic comm. Initialized!");
}

function createSimpleTask(){
  let x= Math.random()* canvas.width;
  let y= Math.random()* canvas.height;
  tasks.push(new Task(x,y));
  logEvent("Spawned simple single-step task by user.");
}

function spawnMultiStepCommanderTask(){
  let x= Math.random()* canvas.width;
  let y= Math.random()* canvas.height;
  let tk= new Task(x,y);
  tk.steps= 1+ Math.floor(Math.random()*3);
  tk.stepCount= tk.steps;
  tk.synergyNeeded= settings.tasks.synergyRequired;
  // multi skill synergy -> random 1..2 topics
  tk.requiredTopics= [];
  let howMany= 1+ Math.floor(Math.random()*2);
  for(let i=0;i< howMany;i++){
    let topID= Math.floor(Math.random()*3);
    if(!tk.requiredTopics.includes(topID)) tk.requiredTopics.push(topID);
  }
  tasks.push(tk);
  logEvent(`Commander synergyTask => steps=${tk.steps}, synergyNeeded=${tk.synergyNeeded}, requiredTopics=${tk.requiredTopics}`);
}

function resetSimulation(){
  initializeSimulation();
  logEvent("Simulation reset!");
}

function updateSimulation(timestamp){
  let delta= timestamp- lastTimestamp;
  lastTimestamp= timestamp;
  fps= (1000/delta).toFixed(1);
  stepTime= delta.toFixed(1);

  timeOfDay+= settings.dayNight.timeRate;
  if(timeOfDay>dayLength) timeOfDay-= dayLength;

  ctx.clearRect(0,0,canvas.width,canvas.height);
  // day/night shading
  ctx.save();
  let nightFactor=0;
  if(timeOfDay>18){
    nightFactor= (timeOfDay-18)/6;
  } else if(timeOfDay<6){
    nightFactor= (6-timeOfDay)/6;
  }
  let alpha= 0.6* nightFactor;
  ctx.fillStyle= `rgba(0,0,0,${alpha})`;
  ctx.fillRect(0,0,canvas.width,canvas.height);
  ctx.restore();

  // threats
  for(const t of threats){
    t.move(timeOfDay);
    t.draw();
  }
  // agents
  for(let i= agents.length-1; i>=0;i--){
    let ag= agents[i];
    if(!ag.alive){
      agents.splice(i,1);
      continue;
    }
    ag.step(agents, threats, foods, tasks, timeOfDay);
    ag.draw();
  }
  // foods
  for(const f of foods){
    f.draw();
  }
  // tasks
  for(const tk of tasks){
    tk.draw();
  }
  // commander
  commander.step(agents, threats, foods, tasks, timeOfDay);

  // UI
  document.getElementById("fps").innerText= fps;
  document.getElementById("stepTime").innerText= stepTime;
  document.getElementById("commanderStep").innerText= commander.stepCount;
  document.getElementById("timeOfDay").innerText= timeOfDay.toFixed(1);

  requestAnimationFrame(updateSimulation);
}

/***********************************************************
   UI hooking
***********************************************************/
document.getElementById("speedSlider").addEventListener("input", e=>{
  let val= parseFloat(e.target.value);
  settings.agent.maxSpeed= val;
  logEvent("Updated agent maxSpeed => "+val);
});
document.getElementById("threatSlider").addEventListener("input", e=>{
  let val= parseFloat(e.target.value);
  settings.threat.aggressiveness= val;
  logEvent("Updated threat aggressiveness => "+val);
});

/***********************************************************
   Start
***********************************************************/
initializeSimulation();
requestAnimationFrame(updateSimulation);

/***********************************************************
   Additional Next Steps:

   1) Expand agent communication to pass partial neural weights
   2) Use knowledgeGraph adjacency in synergy logic (some tasks 
      require certain edges)
   3) Agents could gather "items" or "tools"
   4) Expand partial obs -> ephemeral memory
   5) Possibly use GNN-based policies or hierarchical RL
***********************************************************/
</script>
</body>
</html>
